import json

import chardet
import nest_asyncio
import requests
from bs4 import BeautifulSoup
from g4f.client import Client
from g4f.Provider import Bing, Liaobots, RetryProvider, You, GeminiPro
from tqdm import tqdm
import os
from loguru import logger

nest_asyncio.apply()

TEXT_FOLDER = "auto_text_1"


def use_gpt_for_questions(text: str, question: str):
    os.makedirs(TEXT_FOLDER, exist_ok=True)
    # print(idx, link)

    try:
        client = Client(
            provider=RetryProvider(
                [Bing, Liaobots, You, GeminiPro], shuffle=False
            )
        )
        prompt = """
        You are an expert in extracting phrases from text. You are provided with a conversation between one or more people.
        Your task is to answer the question using ONLY the text provided. The answer must be appropriate to the context.
        You cannot skip words in the answer - the text must go in a row until the end of the answer.
        This task is very important for my career. Take a deep breath and complete the task with all your might.
        You MUST return answer in Russian. Output the answer in JSON format like this:
        {
        "answer": "extracted text"
        }""" + f"""\nQuestion:\n{question}\n\nText:\n"""
        # If you didn't find the answer, you may return {"answer": ""}.

        text = text.strip()  #.replace("\n", " ")
        while "  " in text:
            text = text.replace("  ", " ")
        text = text[:30000]  # setting max content length

        for idddx in range(5):
            curr_text = text[idddx * 7500 : (idddx + 1) * 7500]
            if len(curr_text) < 2000:
                break

            response_gpt = client.chat.completions.create(
                model="",
                messages=[{"role": "user", "content": f"{prompt}{curr_text}"}],
            )
            answer_gpt = response_gpt.choices[0].message.content
            data = json.loads(answer_gpt.replace("```", "").replace("json", ""))
            with open(
                f"{TEXT_FOLDER}/{idddx}.json", "w", encoding="utf-8"
            ) as file:
                json.dump(data, file, ensure_ascii=False, indent=4)
    except Exception as err:
        logger.error(err)


if __name__ == "__main__":
    print("Start pool")
    text = """Мы также продолжим записи. А, окей, да. Соря, он меня что-то перекликивает. Я когда, короче, к нам устроился Влад и Вадима... 
    Вадима постоянно Владом называл. А сейчас у меня обратно переклинило. Вот. Окей, спасибо, Жень. Давайте перейдем к следующему спикеру. 
    Это Тихон Большаков. Тихон, представь, что вы тема, потому что ты на голосовании носил. Ну, хорошо. Так, мне хорошо слышно? Да. Так, видно 
    в презентацию? Да. Всем привет, меня зовут Бойшков Тихон. Сегодня я расскажу теорию Efusion-проволистик-модулс, основные принципы, о которых 
    они построены. И немножко посмотрим примеры. В последнее время мы наблюдаем взрывной рост моделей генерации в высококачественных изображениях 
    из текстовых описаний, а именно модели на основе дефюсионных принципов генерации до момента наиболее популярных. Сегодня мы рассмотрим основные 
    концепции методы в основе Diffusion Models. Начнем с азов. Большинство задач машины обучения, которые мы решаем, основаны на генеративной и 
    дискоменционной моделях. Дискоменционной модели мы обучаем с помощью контролируемого обучения, например, для задач классификации, обнаружения 
    или сегментации. Генеративной же модели с помощью контролируемого или уже неконтролируемого обучения пытаются изучить распределение точек данных 
    в пространстве, например, для генерации новой выборки из обучающих данных. Четыре наиболее хорошие известные алгоритмы генерации представлены на 
    слайде. Это GAN, вориционные автонкодеры, FlowBaseModels и дефизионные модели. Эти модели сначала обучаются, чтобы научиться моделировать распределение 
    данных, а после модель может использовать вот это распределение до создания нового. Обобщенно, каждая архитектура имеет свои достоинства и недостатки. 
    Например, рационов-тенкодеры имеют высокую частоту дискритизации, генерируют разнообразные образцы, но имеют низкое качество генерации образцов. Flow 
    Boost Modules уже генерируют очень разнообразные образцы, но для этого нужна специализированная архитектура и качество генерации образцов оставляет 
    желать ульшого. Ганы имеют высокое качество генерации образцов, но низкое разнообразие образцов на выходе. и Diffusion Models генерируют образцы высокого 
    качества, довольно разнообразные, но имеют низкую скорость работы. Но все знания, полученные при исследованиях перечительных моделей, они привели к столь 
    быстром прогрессу и дефрозионных моделей. Немного физики, диффузия — это нерновесный процесс перемещения, например молекул и атомов в газах, ионов в 
    плазме, электронов в полупроводниках и тому подобное. Вещество из области с высокой концентрацией примечается в область с низкой концентрацией, 
    проводящей к самопроизвольному выравниванию концентрации по всему занимаемому объему. данный процесс управляется случайным движением и по аналогии с 
    прямым процессом как раз дефузиумных неерситивых моделей. Итак, к модели дефузи — это класс вариантастных генеративных моделей, который превращает шум в 
    репрезентативную выборку данных. Используя модели дефузии, мы можем генерируть изображение как условно, так и безусловно. То есть не управляемо, когда 
    модель сама генерирует образец, а также управляемо с помощью подсказок и парантов. Для лучшего понимания теории дефузивенных моделей сосредоточимся на 
    безусловной генерации. Модели дефузии в глубоком обучении были впервые представлены в основополагающей статье 2015 года, но, к сожалению, на какое-то 
    время это осталось заканом. Но в 2015 году была публикована статья «Кенеративное моделирование путем оценки градиент в распределении данных». Используя 
    тот же принцип, что и в статье 2015 года, но другой подход. И уже в 2020 году была публикована статья, ставшая популярно сейчас. Это вероятностная модель 
    депрессионного шумопотавления, сокращенного ДТПМ. После вот этой статьи начались исследования моделей дефузии ужасно-вайцами. За относительно короткого 
    времени был достигнут значительный прогресс создания, обучения и улучшения генеративного моделирования на основе тюфузии. Итак, главная идея дефузионных 
    моделей, взята из физики и гласей, мы можем постепенно преобразовывать одно распределение в другое, используя цепь Markov. Дефузионные генеративные модели 
    состоят из двух противоположных процессов, а именно процессы прямой и обратной диффузии. В процессе прямой диффузии или forward diffusion process мы итеративно 
    обавляем шум, искажая изображение и выводя его из своего существующего подпространства. как представлено на первой картинке. В конце изображение становится совершенно 
    неузнаваемым и выглядит как шум. На этой манике картинки можно увидеть. Цепь Маркова определяется формулой 1. Нормальное Гауссовое распределение определяется формулой 2. 
    Именно оно определяет приходную функцию и является фиксированным. Называется она также и дрова прямой дефузии. Итак, мы берем изображение x0 из неизвестного распределения q и, 
    начиная с временного шага t с фиксированной скоростью бета, зашумляем нормальным голосом распределения. Шум добавляется подобным приемом использованным в вариационных аптенкодерах. 
    Из второго уровня мы можем вывести Sample XT, где epsilon представляет собой тот самый шум, добавляемый от первого до ТТУ шага. На практике раньше устанавливали об этом в тяпозоне от 
    1.00000 до 200.000 и число шагов 1000. Скорость данного неэффективного процесса была очень медленной, так как мы должны пройти через все промежуточные шаги от i-1. Чтобы исправить 
    это, авторы DTPM перепформулировали ядро, чтобы непосредственно переходить в временного шага 0 к шагу t. Для этого определили два уравнения, четыре и пять, где пятое уравнение 
    является аккумулетивным произведением альфа от одного до Т. Далее в эту формуле заменили на альфу и получили уравнение шесть. С помощью вот этого уравнения шестого можно получить 
    сэмпл на любом временном шаге Т 
    цепи Маркова. Ну и, соответственно, это сильно ускорил процесс, это одна из главных дослуг в принципе авторов этой статьи. Перейдем к процессу обратной дефузии. 
    Его идея состоит в том, чтобы обратить процесс ранее описанной прямой с 5. Обратный процесс начинается с момента, когда закончился прямой процесс. Формула Сильмая. 
    Еще в 1949 году был доказан филлер, что для гаосовых распределений обратный процесс имеет ту же функциональную форму, что и прямой. Поэтому с помощью модели глубокого обучения мы 
    на каждом шаге прогнозируем параметры переходного ядра. возвращающего полученной прямом процессе шум в наше исходное пространство данных. Ядро процесса обратной дефузии представляет 
    собой интеграл по всем возможным путям, возвращающим к исходной выборке данных. Данный процесс также можно назвать шумоподавлением, и подход обратного процесса он более стабилен, 
    чем в ганах, и эффективнее, чем в ориционах автоинкондуров и лоб-байст-модвус. В принципе, на сладе представлено общее схема прямого обратного процесса. Тут, в принципе, указаны все 
    формулы, о которых я ранее сказал. То есть эту схему можно использовать общего понимания. Итак, цель обучения дефузионных моделей сводится к максимизации алгоритмической вероятности 
    с генерированной выборки, принадлежащей сходному распределению данных. Что же касается функции потерь, то авторы черпают вдохновение из вариационных авторинкодров и она представляет 
    собой следующее страшное выражение. Но после прощения и игноевирования многих переменов в нем, что, по словам авторов, также улучшила 
    результаты, окончательная loss function стала уравнением внизу слайда. Для меня математика получения данной функции довольно сложна, и мы бы не успели, наверное, 
    её разобрать за время выступления. Ну и вопрос, нужно ли. Поэтому можно назвать, что наиболее впечатляющий вклад авторсататидии TPM – это вывод также вот этой формулы. Ведь начав 
    с той длинной формулы, они перешли к одной из самых простых функций потерь во всей области машин кучения. Что же касается архитектуры модели, то в статье DPM использовалось UNED, 
    который на входе принимает изображение на любой стадии обратного процесса и 
    соответствующий этому изображению временной шаг. Основные компоненты модели – это блоки инкодера, ботлнег-блоки, блоки декодера, self-attention models и sinusoidal 
    time imbalance. В принципе, это вся основная теория чужих модулс, с которой берется основа. Осталось время, поэтому можно в скоте пройти с tdpm, которая снова на статте dpm, в большей 
    степени. Выгодно отличие с tdpm от tdpm, а также ее ближайших конкурентов, доли image и image. в том, что unit внутри stable diffusion не работает с пикселями, а работает с вот этим 
    скрытым латентным пространством изображений, полученным от кодировщика. Это ускоряет процесс и убавляет требования к видеопамисе. В архитектуру также добавляется предобочный кодировщик 
    текста клип для подачи текста, который с помощью блоков attention, так сказать, примешивается к предсказываемому в Unet пространство. В конце уже Dekoder генерирует изображение из полученного 
    пространства. То есть само изображение внутри U-Net никак не участвует, оно на входе кодируется кодировщиком и уже подается на вход U-Net. А внутри U-Net, так как вот это латентное пространство 
    намного меньше, чем картинка, с более высокой скоростью Обрабатывается, и конечный результат на выходе Winnet подавится декорировщик, который эффективно перевозит полученное 
    пространство в изображение. Вот кодировщик-клип является одной из самых важных частей генеративных моделях. Его также используют по Кадинскому 2.1. На слайзе указан пример, который был 
    сгенерирован Кадинским по текстовому запросу Кадинскому на слайзе. В принципе, схема работы клипа в кондинском такайже, то есть в боннинге тексты полученные из клипа передаются в attention, 
    которые примешиваются к выходам резидио блоков. Ну, мы все, наверное, в курсе, как работают внимание в NLP-моделях, воспользовать немножко случаем, чтобы вспомнить, что такое внимание, и также, 
    как этот блок может использоваться не только в NLP-моделях, но и моделях, которые используются для изображений. Дефезионы в моделях также используются и выполняют немалую роль. Здесь представлена 
    обобщенная схема. На входе токены текста с помощью трёх dense-слайов переводится в M-Bending, которую можно называть вектор 
    запрос, вектор ключ и вектор значения. Строится матрица в соответствии всех векторов ключей и векторов запросов. Для пересечения ключей и векторов запросов мы 
    просто считаем произведение этих векторов. Далее берем Softmax по всем строкам. значений, тех же токенов, и также подаем это все на dense-слой и получаем на выходе вектор, размерность 
    которого равна размерности вектора на входе. Из-за того, что вот эти размерности равны, мы можем последовательно применять несколько слоев внимания, получаем при этом так называемый 
    мультиотенческий слой. Это, в принципе, общая схема. Как же можно использовать схему этого внимания применительно к компьютерному зрению? По сути, изображение представляет собой набор пикселей. 
    А часть изображения — это так называемая патча. То есть, грубо говоря, вырезан из этого изображения квадратики, который имеет свою высоту и ширину. Эти патчи можно развернуть по флатон вектор и 
    также использовать как вход на attention. В принципе, так делается, например, в пишем трансформерах, обобщенно. Ну и немножко основлюсь на Ctrl-Net просто для общего развития, что еще есть 
    дефузионная модель Ctrl-Net, которая принимает на входе промтейн, но не видя текстовых каких-то запросов, а может принимать также, например, границы границы объектов, результаты нахождения позисти 
    Мейшон, и это позволяет контролировать выход, то есть выходное изображение, тем самым получая больше то, что мы хотим. То есть словами, например, не все можно описать, какое именно положение человека 
    мы хотим видеть, или где именно заканчиваются там караны, картинки, Если это можно представить в виде границ или ключевых точек, то изображение становится для нас более тем, который мы хотим. В 
    принципе, у меня всё. Спасибо за внимание. Может, есть какие-то вопросы? А, слушай, да, есть на самом деле. Получается в DDPM, если ты не отправляешь туда ничего на вход, да, ну в виде там, промта 
    или вектора какого-то, он будет случайное изображение генерировать из шума? Да, то есть ну это как с ганами работает, если гану подать там пол изображения какой-то и сказать, что это пространство, 
    которым находится эти объекты, то он потом из случайных векторов, ну то есть случайного входа будет формировать картинку из этого распределения. То есть DDPM он Вот как вот на этом стайде было, 
    там генерировался цветок. Детебе им на входе принимал просто пол картинок одного класса, например, цветов. И, соответственно, переводил их какое-то скрытое представление, и учился из этого скрытое 
    представление генерировать те же самые цветы, только уже случайно. Такое-то представление, то в данном случае зашумленное изображение исходное, да? Да, да. Окей. А как тогда 
    управлять этой генерацией? Ты не дошел до этого момента? Управлять генерацией, в смысле... Ну, ты допустил, пусть не цветорик сгенерировать 
    автомобиль там. А, в этом плане, тогда ему надо было подавать паузы, упражнения автомобиль. Ну, у тебя же генератор-то учится генерировать все изображения, 
    ну то есть, в самом говоря, у тебя там есть ImageNet какой-нибудь, да, ты там ImageNet зашумливаешь, там тысячи классов типа, и все тысячи классов-то учишь-то генерировать. Как понять, что я 
    хочу сгенерировать? Ну, допустим, я хочу там автомобили или верблюда сгенерировать. А, в этом плане так. Да, в этом. В кондинском и в Stable Diffusion это понятно, как это сделать. Да, вот как он в 
    помощи клипа. Тут, типа, какое изображение с генерацией определяется с амплированием с голосового распределения, что ли с ходом шумом, или чем-то еще оно определяется. Я так понял, что если ему... Ну, то есть, ДтПМ учился только на одном классе. То есть, ему не было... Ну, на вход, да, но все классы, все классы, которые есть, мы можем найти, допустим. То есть, ему при обучении был дан тук один класс. И он, соответственно, ему учился. То есть, если мы его обучили на цветках, он будет генерировать только цветки. Ну, цветки. То есть, нельзя контролировать, в том, что мы хотим получить на фоне. То есть, это, по сути, определяется чисто этим галсом шумом на фоне, да? Да-да-да. Ну в принципе как в ганах? Ну то есть ганах тоже. Он может как в сеганах 
    там какой-нибудь лейбл как-то... В детских ганах там по-моему же исходный вектор раздаётся какой-то. То есть у тебя условно горячо скрытое пространство, и ты из него можешь темплировать векторы. 
    И типа вектора определяется что-то в генерию. Нет, ты можешь... Ну вот, в сеганах там просто ты можешь вектор садоправить себе, как-то по современному сону горя. Ну да, да, да, вот я про это же. 
    У тебя там есть скрытая пространство этих векторов, из которых ты генеришь, типа, и ты можешь, ну как, вектором сдавать, что ты на генеришь. Ну, ты там что-то подобное тоже можешь. Как твои 
    места промтов, особенно, говорят, то же самое. Только помощь одного от двух лэбов, от лектора. Да, возможно. То есть я именно видел реализацию, где он научался только на одном классе. То есть 
    там не было никакой дополнительной информации, что ему сгенерить. Ну, потому что он один класс только и видел. Кроме говоря, при обучении ты, соответственно, генерировал только образцы этого класса. 
    Вот как, например, ему подать весь EmojiNet и контролировать какую из классов из EmojiNet ему не нерить, такого я не видел. Ну, понятно. Вот, видимо, просто у тебя получается... Ну, да, так же того, как 
    в Dekegane получается, да, только ты исходным шумом контролируешь что-то буждекодировать, а пространство этого шума, оно просто охерить такое огромное. Ну, то есть, по сути, ты не контролируешь. Да, да, да. Ну, в принципе, образцы, они приемлемы, они... высокого качества довольно разнообразно и ну реалистично Смотрите к вопросу. При обратном проходе мы восстанавливаем поэтап на самоизображение или поставили какие-то статьи, что там восстанавливается не самоизображение, но становится шум, который был на шаге на этом. 
    И потом оно вычитается из изображения. Как в оригинальных статьях это делается? Ну вот в DDPM восстанавливается именно изображение. То есть Там UNED учится генерировать ядро перехода от зашумленного 
    изображения к чистому изображению. А если ты генеришь ядро перехода, 
    это же не значит, что ты генеришь само изображение? Ты получаешь генеришь параметр для какой-то алгоритмы? Но нет, как раз с помощью этого ядра можно получить 
    исходное изображение по форму, который я вот вначале показывал. Ну да, да, да, да. Мы генерим шум, грубо говоря, но этот шум, он наоборот, очищает изображение. Ну да, то есть ты из шума, в 
    предыдущем на шаге, вычинаешь шум, который ты сгенерил с помощью юнета и получается более чистое изображение. Да, да, да. Может, ты совсем вычитаешь, да? Ну... Короче, да. Какую-то функцию 
    применяешь между шумом и... с генералом шумом и предыдущей терации получается. Да, да. В принципе, вот все работает по этой первой формуле. Она для прямого подхода и... ну, прохода и для обратного. 
    Т.е. функция, как раз, цепмаркал, который описывает. Ну... Смотри, ты говорил при прямом подходе, мы можем скипнуть, если он говорит о шаге промежуточной. При обратном мы не можем скипнуть. 
    При обратном, как я понял, там это затруднено. Потому что сетка не сможет тебя сразу найти вот это ядро переходное, чтобы сразу чистый изображение получить. То есть там надо это делать последовательно. 
    А в чем смысл скипать шаги на прямом проходе? Ты же сэмпл получаешь, ну, в основном говоря, вот у тебя 1000 шагов, а ты получаешь 6000 сэмплов для обучения. И на обратной операции нужно 1000 
    сэмплов также сделать. В чем смысл скипать? Я так понял, что они скипают не... Вот, например, тысячу шагов у нас есть. Они скипают не всю тысячу шагов, да? Сразу получают изображения на тысячном 
    шаге, они скипают каждую стону, например, шагов. То есть мы можем найти до изображения XT за 10 шагов, вместо 1000. Т.е. смысл именно за меньше промежуточных количеств шагов? Да, шум, делаем там тысячу 
    шагов, а 
    динозим за 100 шагов, допустим, за 10 шагов, да? Но получается у тебя сэмпл, скорее всего, в обычащей выборке будет также 10 или 100. Да, там одинаковое количество. То есть смысл в том, чтобы проскипать 
    большое количество шагов и заменьшить число пройти к зашумленному изображению, либо наоборот, при обратном процессе, заменьше количество шагов прийти к чистому изображению. То здесь скипается каждый 100 
    шагов, мы получаем изображение или зашумленный сэмпл на 100, на 200, на 100, на 1000 шагов. 
    И за это же количество шагов мы учимся проходить в обратном процессе. То есть одинаково количество прямомкой обратно в процессе число шагов. Тяжело получается по цепи Маркова. Прямо и обратный проход. 
    Они же типа имеют одну и ту же формулу, по-моему, да? Что там доказывается в 1949 году? Это получается, да. Это же реформули и по такому же числу шагов можно прийти обратно. Ты получается через эту же формулу, скип шагов можешь рассчитать новую функцию перехода и уже для нового функции перехода генерируете ядрою на этом. Да. То есть у тебя получается обратный проход тоже скипуемый получается, но ты не можешь динамически менять скип, ну как бы если ты обучил его на одной дискритизации этих скипов, да? Да, да, да, так уже не длимать. Там столько же шагов, должно быть. Потому что, кстати, Unetto подаётся на вход не только как бы изображение, да? Ну, именно в DDP, но ещё и временная метка. То есть там штамп, каким это... Ну, каком шаге это изображение сейчас зашумлено. То есть, например, это первый шаг, второй, третий, четвёртый, пятый, всего пять шагов. То есть, на входе мы Unetto подаём то же, что изображение первое, После динозинга. Второе изображение после динозинга, 3 и 4. 
    И за 5 он уже тоже на чистом изображении прийти. Можешь показать схему UNET? У тебя там была схема UNET где-то? Угу. Где тут attention? Мне просто... Я просто смотрю и вижу self attention model. 
    Они его тут после каждого резидиол блока вставляют. После каждого резидела блока. Атеншн с временем получается, да? Или как после резидела блока? Что еще раз? Атеншн с чем получается? На каком этапе? 
    Мы добавляем time embedding после резидела блока. И после этого attention применяем. Да-да, то есть вот этот резидел блок, да, большая плашка. После него идёт Self-Otension. Это типа пиксели с 
    пикселями? Отension что-то получается? Ну, именно в реализации DDPM'а, да. Ну, поэтому это очень много времени уходило. Скорость была очень плохой. Как бы от этого как раз отришили в Stable Diffusion. 
    Там уже отension применяется к вот этому скрытому пространству, у которого размера столько меньше, чем у самого изображения. Это же пиздец дичь. Про кем тебя? 256 на 256. Сколько там элементов в 
    мультикации на 
    Flotention должно быть? Они же тут вряд ли используют такой же Flotention, как в Vity. Возможно, какой-то... 65 тысяч элементов. Йокорами, Бабай, а ну... 
    Это, по-моему, очень тяжелый attention на первых слоях должен быть. Ну, кстати, еще надо было посмотреть для интереса, сколько параметров ДТПМ было, что-то не посмотрел. Ну, вообще, да. Возможно, 
    они, кстати, его на первых слоях и добавляли там. Хотя они еще могут, знаешь, что делать, они могут attention применять по каналу. Типа... Для каждого пикселя вычислять на картов типа вычислять 
    важность того или иного канала. Вот это уже более адекватно получается. У тебя получается тогда 65 тысяч вычисления attention, но каждый attention всего по n-каналам применяется. Есть кого-то вопросы? 
    Так, ну, собственно, если вопросов нет, тогда тихо, спасибо за выступление. Я предлагаю сегодняшний наш обзор статей заканчивать. Напоминаю то, что описание тех статей, которые вы брали за основу выступления, необходимо сделать на эксклауде. Я постараюсь, ну, наверное, уже в понедельник разобраться с доступами, что там за проблемы, да? Вот, ну и будет доступна табличка. Табличку я закрепил в чапкере. Ну, собственно, наверное, всем спасибо. Сегодня не вижу смысла отдавать."""
    question = "составь протокол встречи"
    # question = "про что тут?"
    # question = "какой распорядок дня?"
    use_gpt_for_questions(text, question)
