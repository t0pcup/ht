{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from moviepy.editor import VideoFileClip\n",
    "\n",
    "# delay = []\n",
    "# dir = \"C:/green/ht/data/Видеозаписи/\"\n",
    "# dir_aud = \"C:/green/ht/audio/\"\n",
    "# for f in os.listdir(dir):\n",
    "#     try:\n",
    "#         if os.path.isdir(dir + f) or os.path.isfile(dir_aud + f.replace(\".mp4\", \".mp3\")):\n",
    "#             delay.append(dir + f)\n",
    "#             continue\n",
    "#         file = VideoFileClip(dir + f)\n",
    "#         file.audio.write_audiofile(\n",
    "#             dir_aud + f.replace(\".mp4\", \".mp3\")\n",
    "#         )\n",
    "#     except:\n",
    "#         print(dir + f)\n",
    "\n",
    "# for dr in delay:\n",
    "#     try:\n",
    "#         dr += \"/\"\n",
    "#         for f in os.listdir(dr):\n",
    "#             if os.path.isdir(dr + f) or os.path.isfile(dr + f.replace(\".mp4\", \".mp3\")):\n",
    "#                 continue\n",
    "#             file = VideoFileClip(dr + f)\n",
    "#             file.audio.write_audiofile(dr + f.replace(\".mp4\", \".mp3\"))\n",
    "#     except:\n",
    "#         print(dir + f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\whisper\\lib\\site-packages\\pyannote\\audio\\core\\io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No language specified, language will be first be detected for each audio file (increases inference time).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.2.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\Admin\\.cache\\torch\\whisperx-vad-segmentation.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.2.1+cu118. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Library cublas64_12.dll is not found or cannot be loaded",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 22\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# save model to local path (optional)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# model_dir = \"/path/\"\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# model = whisperx.load_model(MODEL_SIZE, device, compute_type=compute_type, download_root=model_dir)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m audio \u001b[38;5;241m=\u001b[39m whisperx\u001b[38;5;241m.\u001b[39mload_audio(audio_file)\n\u001b[1;32m---> 22\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranscribe\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRU\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msegments\u001b[39m\u001b[38;5;124m\"\u001b[39m])  \u001b[38;5;66;03m# before alignment\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# delete model if low on GPU resources\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# import gc\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m \n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m#python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\Admin\\.cache\\torch\\whisperx-vad-segmentation.bin\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\whisper\\lib\\site-packages\\whisperx\\asr.py:218\u001b[0m, in \u001b[0;36mFasterWhisperPipeline.transcribe\u001b[1;34m(self, audio, batch_size, num_workers, language, task, chunk_size, print_progress, combined_progress)\u001b[0m\n\u001b[0;32m    216\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m batch_size \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_size\n\u001b[0;32m    217\u001b[0m total_segments \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(vad_segments)\n\u001b[1;32m--> 218\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, out \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(data(audio, vad_segments), batch_size\u001b[38;5;241m=\u001b[39mbatch_size, num_workers\u001b[38;5;241m=\u001b[39mnum_workers)):\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m print_progress:\n\u001b[0;32m    220\u001b[0m         base_progress \u001b[38;5;241m=\u001b[39m ((idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m total_segments) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\whisper\\lib\\site-packages\\transformers\\pipelines\\pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\whisper\\lib\\site-packages\\transformers\\pipelines\\pt_utils.py:125\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m    124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[1;32m--> 125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\whisper\\lib\\site-packages\\transformers\\pipelines\\base.py:1102\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1101\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1102\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1103\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\whisper\\lib\\site-packages\\whisperx\\asr.py:152\u001b[0m, in \u001b[0;36mFasterWhisperPipeline._forward\u001b[1;34m(self, model_inputs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_inputs):\n\u001b[1;32m--> 152\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_segment_batched\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m: outputs}\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\whisper\\lib\\site-packages\\whisperx\\asr.py:47\u001b[0m, in \u001b[0;36mWhisperModel.generate_segment_batched\u001b[1;34m(self, features, tokenizer, options, encoder_output)\u001b[0m\n\u001b[0;32m     39\u001b[0m previous_tokens \u001b[38;5;241m=\u001b[39m all_tokens[prompt_reset_since:]\n\u001b[0;32m     40\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_prompt(\n\u001b[0;32m     41\u001b[0m     tokenizer,\n\u001b[0;32m     42\u001b[0m     previous_tokens,\n\u001b[0;32m     43\u001b[0m     without_timestamps\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39mwithout_timestamps,\n\u001b[0;32m     44\u001b[0m     prefix\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39mprefix,\n\u001b[0;32m     45\u001b[0m )\n\u001b[1;32m---> 47\u001b[0m encoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m max_initial_timestamp_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28mround\u001b[39m(options\u001b[38;5;241m.\u001b[39mmax_initial_timestamp \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_precision)\n\u001b[0;32m     51\u001b[0m )\n\u001b[0;32m     53\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m     54\u001b[0m         encoder_output,\n\u001b[0;32m     55\u001b[0m         [prompt] \u001b[38;5;241m*\u001b[39m batch_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     61\u001b[0m         suppress_tokens\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39msuppress_tokens,\n\u001b[0;32m     62\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\whisper\\lib\\site-packages\\whisperx\\asr.py:86\u001b[0m, in \u001b[0;36mWhisperModel.encode\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m     83\u001b[0m     features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(features, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     84\u001b[0m features \u001b[38;5;241m=\u001b[39m faster_whisper\u001b[38;5;241m.\u001b[39mtranscribe\u001b[38;5;241m.\u001b[39mget_ctranslate2_storage(features)\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_cpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mto_cpu\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Library cublas64_12.dll is not found or cannot be loaded"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "import whisperx\n",
    "\n",
    "device = \"cpu\"\n",
    "# device = \"cuda\"  # pip install nvidia-cublas-cu12\n",
    "# audio_file = \"1.mp3\"\n",
    "audio_file = \"1_cut.wav\"\n",
    "batch_size = 2  # reduce if low on GPU mem\n",
    "compute_type = (\"int8\")  # float16 int8\n",
    "RU = \"ru\"\n",
    "MODEL_SIZE = \"small\"  # \"large-v2\"\n",
    "\n",
    "# 1. Transcribe with original whisper (batched)\n",
    "model = whisperx.load_model(MODEL_SIZE, device, compute_type=compute_type)\n",
    "\n",
    "# save model to local path (optional)\n",
    "# model_dir = \"/path/\"\n",
    "# model = whisperx.load_model(MODEL_SIZE, device, compute_type=compute_type, download_root=model_dir)\n",
    "\n",
    "audio = whisperx.load_audio(audio_file)\n",
    "result = model.transcribe(audio, batch_size=batch_size, language=RU)\n",
    "print(result[\"segments\"])  # before alignment\n",
    "\n",
    "# delete model if low on GPU resources\n",
    "# import gc\n",
    "\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "# del model\n",
    "\n",
    "#python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\Admin\\.cache\\torch\\whisperx-vad-segmentation.bin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Мы также продолжим записи. А, окей, да. Соря, он меня что-то перекликивает. Я когда, короче, к нам устроился Влад и Вадима... Вадима постоянно Владом называл. А сейчас у меня обратно переклинило. Вот. Окей, спасибо, Жень. Давайте перейдем к следующему спикеру. Это Тихон Большаков. Тихон, представь, что вы тема, потому что ты на голосовании носил. Ну, хорошо. Так, мне хорошо слышно? Да. Так, видно в презентацию? Да. Всем привет, меня зовут Бойшков Тихон. Сегодня я расскажу теорию Efusion-проволистик-модулс, основные принципы, о которых они построены. И немножко посмотрим примеры. В последнее время мы наблюдаем взрывной рост моделей генерации в высококачественных изображениях из текстовых описаний, а именно модели на основе дефюсионных принципов генерации до момента наиболее популярных. Сегодня мы рассмотрим основные концепции методы в основе Diffusion Models. Начнем с азов. Большинство задач машины обучения, которые мы решаем, основаны на генеративной и дискоменционной моделях. Дискоменционной модели мы обучаем с помощью контролируемого обучения, например, для задач классификации, обнаружения или сегментации. Генеративной же модели с помощью контролируемого или уже неконтролируемого обучения пытаются изучить распределение точек данных в пространстве, например, для генерации новой выборки из обучающих данных. Четыре наиболее хорошие известные алгоритмы генерации представлены на слайде. Это GAN, вориционные автонкодеры, FlowBaseModels и дефизионные модели. Эти модели сначала обучаются, чтобы научиться моделировать распределение данных, а после модель может использовать вот это распределение до создания нового. Обобщенно, каждая архитектура имеет свои достоинства и недостатки. Например, рационов-тенкодеры имеют высокую частоту дискритизации, генерируют разнообразные образцы, но имеют низкое качество генерации образцов. Flow Boost Modules уже генерируют очень разнообразные образцы, но для этого нужна специализированная архитектура и качество генерации образцов оставляет желать ульшого. Ганы имеют высокое качество генерации образцов, но низкое разнообразие образцов на выходе. и Diffusion Models генерируют образцы высокого качества, довольно разнообразные, но имеют низкую скорость работы. Но все знания, полученные при исследованиях перечительных моделей, они привели к столь быстром прогрессу и дефрозионных моделей. Немного физики, диффузия — это нерновесный процесс перемещения, например молекул и атомов в газах, ионов в плазме, электронов в полупроводниках и тому подобное. Вещество из области с высокой концентрацией примечается в область с низкой концентрацией, проводящей к самопроизвольному выравниванию концентрации по всему занимаемому объему. данный процесс управляется случайным движением и по аналогии с прямым процессом как раз дефузиумных неерситивых моделей. Итак, к модели дефузи — это класс вариантастных генеративных моделей, который превращает шум в репрезентативную выборку данных. Используя модели дефузии, мы можем генерируть изображение как условно, так и безусловно. То есть не управляемо, когда модель сама генерирует образец, а также управляемо с помощью подсказок и парантов. Для лучшего понимания теории дефузивенных моделей сосредоточимся на безусловной генерации. Модели дефузии в глубоком обучении были впервые представлены в основополагающей статье 2015 года, но, к сожалению, на какое-то время это осталось заканом. Но в 2015 году была публикована статья «Кенеративное моделирование путем оценки градиент в распределении данных». Используя тот же принцип, что и в статье 2015 года, но другой подход. И уже в 2020 году была публикована статья, ставшая популярно сейчас. Это вероятностная модель депрессионного шумопотавления, сокращенного ДТПМ. После вот этой статьи начались исследования моделей дефузии ужасно-вайцами. За относительно короткого времени был достигнут значительный прогресс создания, обучения и улучшения генеративного моделирования на основе тюфузии. Итак, главная идея дефузионных моделей, взята из физики и гласей, мы можем постепенно преобразовывать одно распределение в другое, используя цепь Markov. Дефузионные генеративные модели состоят из двух противоположных процессов, а именно процессы прямой и обратной диффузии. В процессе прямой диффузии или forward diffusion process мы итеративно добавляем шум, искажая изображение и выводя его из своего существующего подпространства. как представлено на первой картинке. В конце изображение становится совершенно неузнаваемым и выглядит как шум. На этой манике картинки можно увидеть. Цепь Маркова определяется формулой 1. Нормальное Гауссовое распределение определяется формулой 2. Именно оно определяет приходную функцию и является фиксированным. Называется она также и дрова прямой дефузии. Итак, мы берем изображение x0 из неизвестного распределения q и, начиная с временного шага t с фиксированной скоростью бета, зашумляем нормальным голосом распределения. Шум добавляется подобным приемом использованным в вариационных аптенкодерах. Из второго уровня мы можем вывести Sample XT, где epsilon представляет собой тот самый шум, добавляемый от первого до ТТУ шага. На практике раньше устанавливали об этом в тяпозоне от 1.00000 до 200.000 и число шагов 1000. Скорость данного неэффективного процесса была очень медленной, так как мы должны пройти через все промежуточные шаги от i-1. Чтобы исправить это, авторы DTPM перепформулировали ядро, чтобы непосредственно переходить в временного шага 0 к шагу t. Для этого определили два уравнения, четыре и пять, где пятое уравнение является аккумулетивным произведением альфа от одного до Т. Далее в эту формуле заменили на альфу и получили уравнение шесть. С помощью вот этого уравнения шестого можно получить сэмпл на любом временном шаге Т цепи Маркова. Ну и, соответственно, это сильно ускорил процесс, это одна из главных дослуг в принципе авторов этой статьи. Перейдем к процессу обратной дефузии. Его идея состоит в том, чтобы обратить процесс ранее описанной прямой с 5. Обратный процесс начинается с момента, когда закончился прямой процесс. Формула Сильмая. Еще в 1949 году был доказан филлер, что для гаосовых распределений обратный процесс имеет ту же функциональную форму, что и прямой. Поэтому с помощью модели глубокого обучения мы на каждом шаге прогнозируем параметры переходного ядра. возвращающего полученной прямом процессе шум в наше исходное пространство данных. Ядро процесса обратной дефузии представляет собой интеграл по всем возможным путям, возвращающим к исходной выборке данных. Данный процесс также можно назвать шумоподавлением, и подход обратного процесса он более стабилен, чем в ганах, и эффективнее, чем в ориционах автоинкондуров и лоб-байст-модвус. В принципе, на сладе представлено общее схема прямого обратного процесса. Тут, в принципе, указаны все формулы, о которых я ранее сказал. То есть эту схему можно использовать общего понимания. Итак, цель обучения дефузионных моделей сводится к максимизации алгоритмической вероятности с генерированной выборки, принадлежащей сходному распределению данных. Что же касается функции потерь, то авторы черпают вдохновение из вариационных авторинкодров и она представляет собой следующее страшное выражение. Но после прощения и игноевирования многих переменов в нем, что, по словам авторов, также улучшила результаты, окончательная loss function стала уравнением внизу слайда. Для меня математика получения данной функции довольно сложна, и мы бы не успели, наверное, её разобрать за время выступления. Ну и вопрос, нужно ли. Поэтому можно назвать, что наиболее впечатляющий вклад авторсататидии TPM – это вывод также вот этой формулы. Ведь начав с той длинной формулы, они перешли к одной из самых простых функций потерь во всей области машин кучения. Что же касается архитектуры модели, то в статье DPM использовалось UNED, который на входе принимает изображение на любой стадии обратного процесса и соответствующий этому изображению временной шаг. Основные компоненты модели – это блоки инкодера, ботлнег-блоки, блоки декодера, self-attention models и sinusoidal time imbalance. В принципе, это вся основная теория чужих модулс, с которой берется основа. Осталось время, поэтому можно в скоте пройти с tdpm, которая снова на статте dpm, в большей степени. Выгодно отличие с tdpm от tdpm, а также ее ближайших конкурентов, доли image и image. в том, что unit внутри stable diffusion не работает с пикселями, а работает с вот этим скрытым латентным пространством изображений, полученным от кодировщика. Это ускоряет процесс и убавляет требования к видеопамисе. В архитектуру также добавляется предобочный кодировщик текста клип для подачи текста, который с помощью блоков attention, так сказать, примешивается к предсказываемому в Unet пространство. В конце уже Dekoder генерирует изображение из полученного пространства. То есть само изображение внутри U-Net никак не участвует, оно на входе кодируется кодировщиком и уже подается на вход U-Net. А внутри U-Net, так как вот это латентное пространство намного меньше, чем картинка, с более высокой скоростью Обрабатывается, и конечный результат на выходе Winnet подавится декорировщик, который эффективно перевозит полученное пространство в изображение. Вот кодировщик-клип является одной из самых важных частей генеративных моделях. Его также используют по Кадинскому 2.1. На слайзе указан пример, который был сгенерирован Кадинским по текстовому запросу Кадинскому на слайзе. В принципе, схема работы клипа в кондинском такайже, то есть в боннинге тексты полученные из клипа передаются в attention, которые примешиваются к выходам резидио блоков. Ну, мы все, наверное, в курсе, как работают внимание в NLP-моделях, воспользовать немножко случаем, чтобы вспомнить, что такое внимание, и также, как этот блок может использоваться не только в NLP-моделях, но и моделях, которые используются для изображений. Дефезионы в моделях также используются и выполняют немалую роль. Здесь представлена обобщенная схема. На входе токены текста с помощью трёх dense-слайов переводится в M-Bending, которую можно называть вектор запрос, вектор ключ и вектор значения. Строится матрица в соответствии всех векторов ключей и векторов запросов. Для пересечения ключей и векторов запросов мы просто считаем произведение этих векторов. Далее берем Softmax по всем строкам. значений, тех же токенов, и также подаем это все на dense-слой и получаем на выходе вектор, размерность которого равна размерности вектора на входе. Из-за того, что вот эти размерности равны, мы можем последовательно применять несколько слоев внимания, получаем при этом так называемый мультиотенческий слой. Это, в принципе, общая схема. Как же можно использовать схему этого внимания применительно к компьютерному зрению? По сути, изображение представляет собой набор пикселей. А часть изображения — это так называемая патча. То есть, грубо говоря, вырезан из этого изображения квадратики, который имеет свою высоту и ширину. Эти патчи можно развернуть по флатон вектор и также использовать как вход на attention. В принципе, так делается, например, в пишем трансформерах, обобщенно. Ну и немножко основлюсь на Ctrl-Net просто для общего развития, что еще есть дефузионная модель Ctrl-Net, которая принимает на входе промтейн, но не видя текстовых каких-то запросов, а может принимать также, например, границы границы объектов, результаты нахождения позисти Мейшон, и это позволяет контролировать выход, то есть выходное изображение, тем самым получая больше то, что мы хотим. То есть словами, например, не все можно описать, какое именно положение человека мы хотим видеть, или где именно заканчиваются там караны, картинки, Если это можно представить в виде границ или ключевых точек, то изображение становится для нас более тем, который мы хотим. В принципе, у меня всё. Спасибо за внимание. Может, есть какие-то вопросы? А, слушай, да, есть на самом деле. Получается в DDPM, если ты не отправляешь туда ничего на вход, да, ну в виде там, промта или вектора какого-то, он будет случайное изображение генерировать из шума? Да, то есть ну это как с ганами работает, если гану подать там пол изображения какой-то и сказать, что это пространство, которым находится эти объекты, то он потом из случайных векторов, ну то есть случайного входа будет формировать картинку из этого распределения. То есть DDPM он Вот как вот на этом стайде было, там генерировался цветок. Детебе им на входе принимал просто пол картинок одного класса, например, цветов. И, соответственно, переводил их какое-то скрытое представление, и учился из этого скрытое представление генерировать те же самые цветы, только уже случайно. Такое-то представление, то в данном случае зашумленное изображение исходное, да? Да, да. Окей. А как тогда управлять этой генерацией? Ты не дошел до этого момента? Управлять генерацией, в смысле... Ну, ты допустил, пусть не цветорик сгенерировать автомобиль там. А, в этом плане, тогда ему надо было подавать паузы, упражнения автомобиль. Ну, у тебя же генератор-то учится генерировать все изображения, ну то есть, в самом говоря, у тебя там есть ImageNet какой-нибудь, да, ты там ImageNet зашумливаешь, там тысячи классов типа, и все тысячи классов-то учишь-то генерировать. Как понять, что я хочу сгенерировать? Ну, допустим, я хочу там автомобили или верблюда сгенерировать. А, в этом плане так. Да, в этом. В кондинском и в Stable Diffusion это понятно, как это сделать. Да, вот как он в помощи клипа. Тут, типа, какое изображение с генерацией определяется с амплированием с голосового распределения, что ли с ходом шумом, или чем-то еще оно определяется. Я так понял, что если ему... Ну, то есть, ДтПМ учился только на одном классе. То есть, ему не было... Ну, на вход, да, но все классы, все классы, которые есть, мы можем найти, допустим. То есть, ему при обучении был дан тук один класс. И он, соответственно, ему учился. То есть, если мы его обучили на цветках, он будет генерировать только цветки. Ну, цветки. То есть, нельзя контролировать, в том, что мы хотим получить на фоне. То есть, это, по сути, определяется чисто этим галсом шумом на фоне, да? Да-да-да. Ну в принципе как в ганах? Ну то есть ганах тоже. Он может как в сеганах там какой-нибудь лейбл как-то... В детских ганах там по-моему же исходный вектор раздаётся какой-то. То есть у тебя условно горячо скрытое пространство, и ты из него можешь темплировать векторы. И типа вектора определяется что-то в генерию. Нет, ты можешь... Ну вот, в сеганах там просто ты можешь вектор садоправить себе, как-то по современному сону горя. Ну да, да, да, вот я про это же. У тебя там есть скрытая пространство этих векторов, из которых ты генеришь, типа, и ты можешь, ну как, вектором сдавать, что ты на генеришь. Ну, ты там что-то подобное тоже можешь. Как твои места промтов, особенно, говорят, то же самое. Только помощь одного от двух лэбов, от лектора. Да, возможно. То есть я именно видел реализацию, где он научался только на одном классе. То есть там не было никакой дополнительной информации, что ему сгенерить. Ну, потому что он один класс только и видел. Кроме говоря, при обучении ты, соответственно, генерировал только образцы этого класса. Вот как, например, ему подать весь EmojiNet и контролировать какую из классов из EmojiNet ему не нерить, такого я не видел. Ну, понятно. Вот, видимо, просто у тебя получается... Ну, да, так же того, как в Dekegane получается, да, только ты исходным шумом контролируешь что-то буждекодировать, а пространство этого шума, оно просто охерить такое огромное. Ну, то есть, по сути, ты не контролируешь. Да, да, да. Ну, в принципе, образцы, они приемлемы, они... высокого качества довольно разнообразно и ну реалистично Смотрите к вопросу. При обратном проходе мы восстанавливаем поэтап на самоизображение или поставили какие-то статьи, что там восстанавливается не самоизображение, но становится шум, который был на шаге на этом. И потом оно вычитается из изображения. Как в оригинальных статьях это делается? Ну вот в DDPM восстанавливается именно изображение. То есть Там UNED учится генерировать ядро перехода от зашумленного изображения к чистому изображению. А если ты генеришь ядро перехода, это же не значит, что ты генеришь само изображение? Ты получаешь генеришь параметр для какой-то алгоритмы? Но нет, как раз с помощью этого ядра можно получить исходное изображение по форму, который я вот вначале показывал. Ну да, да, да, да. Мы генерим шум, грубо говоря, но этот шум, он наоборот, очищает изображение. Ну да, то есть ты из шума, в предыдущем на шаге, вычинаешь шум, который ты сгенерил с помощью юнета и получается более чистое изображение. Да, да, да. Может, ты совсем вычитаешь, да? Ну... Короче, да. Какую-то функцию применяешь между шумом и... с генералом шумом и предыдущей терации получается. Да, да. В принципе, вот все работает по этой первой формуле. Она для прямого подхода и... ну, прохода и для обратного. Т.е. функция, как раз, цепмаркал, который описывает. Ну... Смотри, ты говорил при прямом подходе, мы можем скипнуть, если он говорит о шаге промежуточной. При обратном мы не можем скипнуть. При обратном, как я понял, там это затруднено. Потому что сетка не сможет тебя сразу найти вот это ядро переходное, чтобы сразу чистый изображение получить. То есть там надо это делать последовательно. А в чем смысл скипать шаги на прямом проходе? Ты же сэмпл получаешь, ну, в основном говоря, вот у тебя 1000 шагов, а ты получаешь 6000 сэмплов для обучения. И на обратной операции нужно 1000 сэмплов также сделать. В чем смысл скипать? Я так понял, что они скипают не... Вот, например, тысячу шагов у нас есть. Они скипают не всю тысячу шагов, да? Сразу получают изображения на тысячном шаге, они скипают каждую стону, например, шагов. То есть мы можем найти до изображения XT за 10 шагов, вместо 1000. Т.е. смысл именно за меньше промежуточных количеств шагов? Да, шум, делаем там тысячу шагов, а динозим за 100 шагов, допустим, за 10 шагов, да? Но получается у тебя сэмпл, скорее всего, в обычащей выборке будет также 10 или 100. Да, там одинаковое количество. То есть смысл в том, чтобы проскипать большое количество шагов и заменьшить число пройти к зашумленному изображению, либо наоборот, при обратном процессе, заменьше количество шагов прийти к чистому изображению. То здесь скипается каждый 100 шагов, мы получаем изображение или зашумленный сэмпл на 100, на 200, на 100, на 1000 шагов. И за это же количество шагов мы учимся проходить в обратном процессе. То есть одинаково количество прямомкой обратно в процессе число шагов. Тяжело получается по цепи Маркова. Прямо и обратный проход. Они же типа имеют одну и ту же формулу, по-моему, да? Что там доказывается в 1949 году? Это получается, да. Это же реформули и по такому же числу шагов можно прийти обратно. Ты получается через эту же формулу, скип шагов можешь рассчитать новую функцию перехода и уже для нового функции перехода генерируете ядрою на этом. Да. То есть у тебя получается обратный проход тоже скипуемый получается, но ты не можешь динамически менять скип, ну как бы если ты обучил его на одной дискритизации этих скипов, да? Да, да, да, так уже не длимать. Там столько же шагов, должно быть. Потому что, кстати, Unetto подаётся на вход не только как бы изображение, да? Ну, именно в DDP, но ещё и временная метка. То есть там штамп, каким это... Ну, каком шаге это изображение сейчас зашумлено. То есть, например, это первый шаг, второй, третий, четвёртый, пятый, всего пять шагов. То есть, на входе мы Unetto подаём то же, что изображение первое, После динозинга. Второе изображение после динозинга, 3 и 4. И за 5 он уже тоже на чистом изображении прийти. Можешь показать схему UNET? У тебя там была схема UNET где-то? Угу. Где тут attention? Мне просто... Я просто смотрю и вижу self attention model. Они его тут после каждого резидиол блока вставляют. После каждого резидела блока. Атеншн с временем получается, да? Или как после резидела блока? Что еще раз? Атеншн с чем получается? На каком этапе? Мы добавляем time embedding после резидела блока. И после этого attention применяем. Да-да, то есть вот этот резидел блок, да, большая плашка. После него идёт Self-Otension. Это типа пиксели с пикселями? Отension что-то получается? Ну, именно в реализации DDPM'а, да. Ну, поэтому это очень много времени уходило. Скорость была очень плохой. Как бы от этого как раз отришили в Stable Diffusion. Там уже отension применяется к вот этому скрытому пространству, у которого размера столько меньше, чем у самого изображения. Это же пиздец дичь. Про кем тебя? 256 на 256. Сколько там элементов в мультикации на Flotention должно быть? Они же тут вряд ли используют такой же Flotention, как в Vity. Возможно, какой-то... 65 тысяч элементов. Йокорами, Бабай, а ну... Это, по-моему, очень тяжелый attention на первых слоях должен быть. Ну, кстати, еще надо было посмотреть для интереса, сколько параметров ДТПМ было, что-то не посмотрел. Ну, вообще, да. Возможно, они, кстати, его на первых слоях и добавляли там. Хотя они еще могут, знаешь, что делать, они могут attention применять по каналу. Типа... Для каждого пикселя вычислять на картов типа вычислять важность того или иного канала. Вот это уже более адекватно получается. У тебя получается тогда 65 тысяч вычисления attention, но каждый attention всего по n-каналам применяется. Есть кого-то вопросы? Так, ну, собственно, если вопросов нет, тогда тихо, спасибо за выступление. Я предлагаю сегодняшний наш обзор статей заканчивать. Напоминаю то, что описание тех статей, которые вы брали за основу выступления, необходимо сделать на эксклауде. Я постараюсь, ну, наверное, уже в понедельник разобраться с доступами, что там за проблемы, да? Вот, ну и будет доступна табличка. Табличку я закрепил в чапкере. Ну, собственно, наверное, всем спасибо. Сегодня не вижу смысла отдавать.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = \"\"\n",
    "\n",
    "for s in result['segments']:\n",
    "    ss += \" \" + s['text'].strip()\n",
    "\n",
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['segments', 'language'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at jonatasgrosman/wav2vec2-large-xlsr-53-russian were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at jonatasgrosman/wav2vec2-large-xlsr-53-russian and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'start': 0.089, 'end': 1.21, 'text': ' Мы также продолжим запись.', 'words': [{'word': 'Мы', 'start': 0.089, 'end': 0.269, 'score': 0.524}, {'word': 'также', 'start': 0.289, 'end': 0.529, 'score': 0.806}, {'word': 'продолжим', 'start': 0.549, 'end': 0.83, 'score': 0.718}, {'word': 'запись.', 'start': 0.87, 'end': 1.21, 'score': 0.851}]}, {'start': 17.734, 'end': 19.574, 'text': ' А, окей, да.', 'words': [{'word': 'А,', 'start': 17.734, 'end': 17.814, 'score': 0.804}, {'word': 'окей,', 'start': 17.894, 'end': 18.314, 'score': 0.921}, {'word': 'да.', 'start': 18.354, 'end': 19.574, 'score': 0.785}]}, {'start': 19.674, 'end': 21.535, 'text': 'Соря, он меня что-то перекликивает.', 'words': [{'word': 'Соря,', 'start': 19.674, 'end': 19.914, 'score': 0.926}, {'word': 'он', 'start': 19.954, 'end': 20.034, 'score': 0.283}, {'word': 'меня', 'start': 20.054, 'end': 20.175, 'score': 0.358}, {'word': 'что-то', 'start': 20.195, 'end': 20.455, 'score': 0.494}, {'word': 'перекликивает.', 'start': 20.495, 'end': 21.535, 'score': 0.901}]}, {'start': 21.635, 'end': 28.116, 'text': 'Я когда, короче, к нам устроился Влад и Вадима... Вадима постоянно Владом называл.', 'words': [{'word': 'Я', 'start': 21.635, 'end': 22.055, 'score': 0.95}, {'word': 'когда,', 'start': 22.115, 'end': 22.435, 'score': 0.96}, {'word': 'короче,', 'start': 22.475, 'end': 24.175, 'score': 0.859}, {'word': 'к', 'start': 24.255, 'end': 24.275, 'score': 0.996}, {'word': 'нам', 'start': 24.315, 'end': 24.455, 'score': 0.802}, {'word': 'устроился', 'start': 24.475, 'end': 24.915, 'score': 0.74}, {'word': 'Влад', 'start': 24.935, 'end': 25.216, 'score': 0.949}, {'word': 'и', 'start': 25.256, 'end': 25.296, 'score': 0.981}, {'word': 'Вадима...', 'start': 25.336, 'end': 25.956, 'score': 0.79}, {'word': 'Вадима', 'start': 26.016, 'end': 26.316, 'score': 0.648}, {'word': 'постоянно', 'start': 26.336, 'end': 26.636, 'score': 0.408}, {'word': 'Владом', 'start': 26.656, 'end': 26.976, 'score': 0.788}, {'word': 'называл.', 'start': 27.016, 'end': 28.116, 'score': 0.937}]}, {'start': 28.136, 'end': 30.237, 'text': 'А сейчас у меня обратно переклинило.', 'words': [{'word': 'А', 'start': 28.136, 'end': 28.156, 'score': 0.335}, {'word': 'сейчас', 'start': 28.236, 'end': 28.396, 'score': 0.199}, {'word': 'у', 'start': 28.416, 'end': 28.456, 'score': 0.499}, {'word': 'меня', 'start': 28.496, 'end': 28.696, 'score': 0.348}, {'word': 'обратно', 'start': 28.716, 'end': 29.056, 'score': 0.854}, {'word': 'переклинило.', 'start': 29.096, 'end': 30.237, 'score': 0.814}]}, {'start': 30.297, 'end': 31.357, 'text': 'Вот.', 'words': [{'word': 'Вот.', 'start': 30.297, 'end': 31.357, 'score': 0.985}]}, {'start': 31.437, 'end': 34.197, 'text': 'Окей, спасибо, Жень.', 'words': [{'word': 'Окей,', 'start': 31.437, 'end': 32.137, 'score': 0.943}, {'word': 'спасибо,', 'start': 32.157, 'end': 32.497, 'score': 0.839}, {'word': 'Жень.', 'start': 32.537, 'end': 34.197, 'score': 0.979}]}, {'start': 34.237, 'end': 36.118, 'text': 'Давайте перейдем к следующему спикеру.', 'words': [{'word': 'Давайте', 'start': 34.237, 'end': 34.497, 'score': 0.801}, {'word': 'перейдем', 'start': 34.537, 'end': 34.777, 'score': 0.502}, {'word': 'к', 'start': 34.797, 'end': 34.817, 'score': 0.975}, {'word': 'следующему', 'start': 34.857, 'end': 35.218, 'score': 0.588}, {'word': 'спикеру.', 'start': 35.298, 'end': 36.118, 'score': 0.87}]}, {'start': 36.138, 'end': 37.898, 'text': 'Это Тихон Большаков.', 'words': [{'word': 'Это', 'start': 36.138, 'end': 36.278, 'score': 0.877}, {'word': 'Тихон', 'start': 36.318, 'end': 36.678, 'score': 0.899}, {'word': 'Большаков.', 'start': 36.738, 'end': 37.898, 'score': 0.812}]}, {'start': 37.958, 'end': 41.279, 'text': 'Тихон, представь, что вы тема, потому что ты на голосовании носил.', 'words': [{'word': 'Тихон,', 'start': 37.958, 'end': 38.358, 'score': 0.768}, {'word': 'представь,', 'start': 38.378, 'end': 38.638, 'score': 0.416}, {'word': 'что', 'start': 38.658, 'end': 38.798, 'score': 0.478}, {'word': 'вы', 'start': 38.818, 'end': 38.858, 'score': 0.0}, {'word': 'тема,', 'start': 38.898, 'end': 39.138, 'score': 0.924}, {'word': 'потому', 'start': 39.178, 'end': 39.398, 'score': 0.664}, {'word': 'что', 'start': 39.438, 'end': 39.578, 'score': 0.996}, {'word': 'ты', 'start': 39.598, 'end': 39.638, 'score': 0.0}, {'word': 'на', 'start': 39.658, 'end': 39.718, 'score': 0.786}, {'word': 'голосовании', 'start': 39.738, 'end': 40.139, 'score': 0.881}, {'word': 'носил.', 'start': 40.199, 'end': 41.279, 'score': 0.995}]}, {'start': 41.299, 'end': 42.819, 'text': 'Ну, хорошо.', 'words': [{'word': 'Ну,', 'start': 41.299, 'end': 41.359, 'score': 0.238}, {'word': 'хорошо.', 'start': 41.399, 'end': 42.819, 'score': 0.684}]}, {'start': 42.879, 'end': 45.74, 'text': 'Так, мне хорошо слышно?', 'words': [{'word': 'Так,', 'start': 42.879, 'end': 43.119, 'score': 0.9}, {'word': 'мне', 'start': 43.179, 'end': 43.399, 'score': 0.931}, {'word': 'хорошо', 'start': 43.459, 'end': 43.719, 'score': 0.965}, {'word': 'слышно?', 'start': 43.739, 'end': 45.74, 'score': 0.612}]}, {'start': 45.84, 'end': 45.9, 'text': 'Да.', 'words': [{'word': 'Да.', 'start': 45.84, 'end': 45.9, 'score': 0.612}]}, {'start': 49.588, 'end': 51.229, 'text': ' Так, видно презентацию?', 'words': [{'word': 'Так,', 'start': 49.588, 'end': 49.848, 'score': 0.826}, {'word': 'видно', 'start': 50.289, 'end': 50.669, 'score': 0.975}, {'word': 'презентацию?', 'start': 50.729, 'end': 51.229, 'score': 0.626}]}, {'start': 52.911, 'end': 54.972, 'text': 'Да.', 'words': [{'word': 'Да.', 'start': 52.911, 'end': 54.972, 'score': 0.978}]}, {'start': 54.992, 'end': 58.155, 'text': 'Всем привет, меня зовут Большоков Тихон.', 'words': [{'word': 'Всем', 'start': 54.992, 'end': 55.212, 'score': 0.923}, {'word': 'привет,', 'start': 55.252, 'end': 55.813, 'score': 0.978}, {'word': 'меня', 'start': 55.853, 'end': 55.993, 'score': 0.655}, {'word': 'зовут', 'start': 56.033, 'end': 56.153, 'score': 0.363}, {'word': 'Большоков', 'start': 56.173, 'end': 56.533, 'score': 0.596}, {'word': 'Тихон.', 'start': 56.593, 'end': 58.155, 'score': 0.976}]}, {'start': 58.215, 'end': 61.357, 'text': 'Сегодня я расскажу теорию EFusion Pro Ballistic Models, основные принципы', 'words': [{'word': 'Сегодня', 'start': 58.215, 'end': 58.455, 'score': 0.404}, {'word': 'я', 'start': 58.515, 'end': 58.875, 'score': 0.726}, {'word': 'расскажу', 'start': 58.935, 'end': 59.636, 'score': 0.979}, {'word': 'теорию', 'start': 59.696, 'end': 60.496, 'score': 0.93}, {'word': 'EFusion'}, {'word': 'Pro'}, {'word': 'Ballistic'}, {'word': 'Models,'}, {'word': 'основные', 'start': 60.616, 'end': 60.817, 'score': 0.124}, {'word': 'принципы', 'start': 60.837, 'end': 61.357, 'score': 0.514}]}]\n",
      "                              segment label     speaker      start        end  \\\n",
      "0   [ 00:00:00.008 -->  00:00:01.196]     A  SPEAKER_02   0.008489   1.196944   \n",
      "1   [ 00:00:10.551 -->  00:00:14.185]     B  SPEAKER_02  10.551783  14.185059   \n",
      "2   [ 00:00:17.682 -->  00:00:19.006]     C  SPEAKER_02  17.682513  19.006791   \n",
      "3   [ 00:00:19.006 -->  00:00:40.517]     D  SPEAKER_00  19.006791  40.517827   \n",
      "4   [ 00:00:40.517 -->  00:00:40.534]     E  SPEAKER_02  40.517827  40.534805   \n",
      "5   [ 00:00:41.366 -->  00:00:41.791]     F  SPEAKER_01  41.366723  41.791171   \n",
      "6   [ 00:00:41.791 -->  00:00:42.843]     G  SPEAKER_02  41.791171  42.843803   \n",
      "7   [ 00:00:42.843 -->  00:00:42.996]     H  SPEAKER_01  42.843803  42.996604   \n",
      "8   [ 00:00:42.996 -->  00:00:44.117]     I  SPEAKER_02  42.996604  44.117148   \n",
      "9   [ 00:00:45.780 -->  00:00:46.171]     J  SPEAKER_02  45.780985  46.171477   \n",
      "10  [ 00:00:47.122 -->  00:00:47.512]     K  SPEAKER_02  47.122241  47.512733   \n",
      "11  [ 00:00:49.533 -->  00:00:50.212]     L  SPEAKER_02  49.533107  50.212224   \n",
      "12  [ 00:00:50.212 -->  00:00:51.281]     M  SPEAKER_01  50.212224  51.281834   \n",
      "13  [ 00:00:52.860 -->  00:00:53.030]     N  SPEAKER_02  52.860781  53.030560   \n",
      "14  [ 00:00:54.303 -->  00:00:57.750]     O  SPEAKER_01  54.303905  57.750424   \n",
      "15  [ 00:00:58.140 -->  00:01:00.246]     P  SPEAKER_01  58.140917  60.246180   \n",
      "16  [ 00:01:00.483 -->  00:01:02.232]     Q  SPEAKER_01  60.483871  62.232598   \n",
      "17  [ 00:01:02.775 -->  00:01:03.998]     R  SPEAKER_01  62.775891  63.998302   \n",
      "\n",
      "    intersection      union  \n",
      "0     -59.640056  61.348511  \n",
      "1     -46.651941  50.805217  \n",
      "2     -41.830209  43.674487  \n",
      "3     -20.319173  42.350209  \n",
      "4     -20.302195  20.839173  \n",
      "5     -19.045829  19.990277  \n",
      "6     -17.993197  19.565829  \n",
      "7     -17.840396  18.513197  \n",
      "8     -16.719852  18.360396  \n",
      "9     -14.665523  15.576015  \n",
      "10    -13.324267  14.234759  \n",
      "11    -10.624776  11.823893  \n",
      "12     -9.555166  11.144776  \n",
      "13     -7.806440   8.496219  \n",
      "14     -3.086576   7.053095  \n",
      "15     -0.590820   3.216083  \n",
      "16      0.520000   1.748727  \n",
      "17     -1.418891   3.161302  \n",
      "[{'start': 0.089, 'end': 1.21, 'text': ' Мы также продолжим запись.', 'words': [{'word': 'Мы', 'start': 0.089, 'end': 0.269, 'score': 0.524, 'speaker': 'SPEAKER_02'}, {'word': 'также', 'start': 0.289, 'end': 0.529, 'score': 0.806, 'speaker': 'SPEAKER_02'}, {'word': 'продолжим', 'start': 0.549, 'end': 0.83, 'score': 0.718, 'speaker': 'SPEAKER_02'}, {'word': 'запись.', 'start': 0.87, 'end': 1.21, 'score': 0.851, 'speaker': 'SPEAKER_02'}], 'speaker': 'SPEAKER_02'}, {'start': 17.734, 'end': 19.574, 'text': ' А, окей, да.', 'words': [{'word': 'А,', 'start': 17.734, 'end': 17.814, 'score': 0.804, 'speaker': 'SPEAKER_02'}, {'word': 'окей,', 'start': 17.894, 'end': 18.314, 'score': 0.921, 'speaker': 'SPEAKER_02'}, {'word': 'да.', 'start': 18.354, 'end': 19.574, 'score': 0.785, 'speaker': 'SPEAKER_02'}], 'speaker': 'SPEAKER_02'}, {'start': 19.674, 'end': 21.535, 'text': 'Соря, он меня что-то перекликивает.', 'words': [{'word': 'Соря,', 'start': 19.674, 'end': 19.914, 'score': 0.926, 'speaker': 'SPEAKER_00'}, {'word': 'он', 'start': 19.954, 'end': 20.034, 'score': 0.283, 'speaker': 'SPEAKER_00'}, {'word': 'меня', 'start': 20.054, 'end': 20.175, 'score': 0.358, 'speaker': 'SPEAKER_00'}, {'word': 'что-то', 'start': 20.195, 'end': 20.455, 'score': 0.494, 'speaker': 'SPEAKER_00'}, {'word': 'перекликивает.', 'start': 20.495, 'end': 21.535, 'score': 0.901, 'speaker': 'SPEAKER_00'}], 'speaker': 'SPEAKER_00'}, {'start': 21.635, 'end': 28.116, 'text': 'Я когда, короче, к нам устроился Влад и Вадима... Вадима постоянно Владом называл.', 'words': [{'word': 'Я', 'start': 21.635, 'end': 22.055, 'score': 0.95, 'speaker': 'SPEAKER_00'}, {'word': 'когда,', 'start': 22.115, 'end': 22.435, 'score': 0.96, 'speaker': 'SPEAKER_00'}, {'word': 'короче,', 'start': 22.475, 'end': 24.175, 'score': 0.859, 'speaker': 'SPEAKER_00'}, {'word': 'к', 'start': 24.255, 'end': 24.275, 'score': 0.996, 'speaker': 'SPEAKER_00'}, {'word': 'нам', 'start': 24.315, 'end': 24.455, 'score': 0.802, 'speaker': 'SPEAKER_00'}, {'word': 'устроился', 'start': 24.475, 'end': 24.915, 'score': 0.74, 'speaker': 'SPEAKER_00'}, {'word': 'Влад', 'start': 24.935, 'end': 25.216, 'score': 0.949, 'speaker': 'SPEAKER_00'}, {'word': 'и', 'start': 25.256, 'end': 25.296, 'score': 0.981, 'speaker': 'SPEAKER_00'}, {'word': 'Вадима...', 'start': 25.336, 'end': 25.956, 'score': 0.79, 'speaker': 'SPEAKER_00'}, {'word': 'Вадима', 'start': 26.016, 'end': 26.316, 'score': 0.648, 'speaker': 'SPEAKER_00'}, {'word': 'постоянно', 'start': 26.336, 'end': 26.636, 'score': 0.408, 'speaker': 'SPEAKER_00'}, {'word': 'Владом', 'start': 26.656, 'end': 26.976, 'score': 0.788, 'speaker': 'SPEAKER_00'}, {'word': 'называл.', 'start': 27.016, 'end': 28.116, 'score': 0.937, 'speaker': 'SPEAKER_00'}], 'speaker': 'SPEAKER_00'}, {'start': 28.136, 'end': 30.237, 'text': 'А сейчас у меня обратно переклинило.', 'words': [{'word': 'А', 'start': 28.136, 'end': 28.156, 'score': 0.335, 'speaker': 'SPEAKER_00'}, {'word': 'сейчас', 'start': 28.236, 'end': 28.396, 'score': 0.199, 'speaker': 'SPEAKER_00'}, {'word': 'у', 'start': 28.416, 'end': 28.456, 'score': 0.499, 'speaker': 'SPEAKER_00'}, {'word': 'меня', 'start': 28.496, 'end': 28.696, 'score': 0.348, 'speaker': 'SPEAKER_00'}, {'word': 'обратно', 'start': 28.716, 'end': 29.056, 'score': 0.854, 'speaker': 'SPEAKER_00'}, {'word': 'переклинило.', 'start': 29.096, 'end': 30.237, 'score': 0.814, 'speaker': 'SPEAKER_00'}], 'speaker': 'SPEAKER_00'}, {'start': 30.297, 'end': 31.357, 'text': 'Вот.', 'words': [{'word': 'Вот.', 'start': 30.297, 'end': 31.357, 'score': 0.985, 'speaker': 'SPEAKER_00'}], 'speaker': 'SPEAKER_00'}, {'start': 31.437, 'end': 34.197, 'text': 'Окей, спасибо, Жень.', 'words': [{'word': 'Окей,', 'start': 31.437, 'end': 32.137, 'score': 0.943, 'speaker': 'SPEAKER_00'}, {'word': 'спасибо,', 'start': 32.157, 'end': 32.497, 'score': 0.839, 'speaker': 'SPEAKER_00'}, {'word': 'Жень.', 'start': 32.537, 'end': 34.197, 'score': 0.979, 'speaker': 'SPEAKER_00'}], 'speaker': 'SPEAKER_00'}, {'start': 34.237, 'end': 36.118, 'text': 'Давайте перейдем к следующему спикеру.', 'words': [{'word': 'Давайте', 'start': 34.237, 'end': 34.497, 'score': 0.801, 'speaker': 'SPEAKER_00'}, {'word': 'перейдем', 'start': 34.537, 'end': 34.777, 'score': 0.502, 'speaker': 'SPEAKER_00'}, {'word': 'к', 'start': 34.797, 'end': 34.817, 'score': 0.975, 'speaker': 'SPEAKER_00'}, {'word': 'следующему', 'start': 34.857, 'end': 35.218, 'score': 0.588, 'speaker': 'SPEAKER_00'}, {'word': 'спикеру.', 'start': 35.298, 'end': 36.118, 'score': 0.87, 'speaker': 'SPEAKER_00'}], 'speaker': 'SPEAKER_00'}, {'start': 36.138, 'end': 37.898, 'text': 'Это Тихон Большаков.', 'words': [{'word': 'Это', 'start': 36.138, 'end': 36.278, 'score': 0.877, 'speaker': 'SPEAKER_00'}, {'word': 'Тихон', 'start': 36.318, 'end': 36.678, 'score': 0.899, 'speaker': 'SPEAKER_00'}, {'word': 'Большаков.', 'start': 36.738, 'end': 37.898, 'score': 0.812, 'speaker': 'SPEAKER_00'}], 'speaker': 'SPEAKER_00'}, {'start': 37.958, 'end': 41.279, 'text': 'Тихон, представь, что вы тема, потому что ты на голосовании носил.', 'words': [{'word': 'Тихон,', 'start': 37.958, 'end': 38.358, 'score': 0.768, 'speaker': 'SPEAKER_00'}, {'word': 'представь,', 'start': 38.378, 'end': 38.638, 'score': 0.416, 'speaker': 'SPEAKER_00'}, {'word': 'что', 'start': 38.658, 'end': 38.798, 'score': 0.478, 'speaker': 'SPEAKER_00'}, {'word': 'вы', 'start': 38.818, 'end': 38.858, 'score': 0.0, 'speaker': 'SPEAKER_00'}, {'word': 'тема,', 'start': 38.898, 'end': 39.138, 'score': 0.924, 'speaker': 'SPEAKER_00'}, {'word': 'потому', 'start': 39.178, 'end': 39.398, 'score': 0.664, 'speaker': 'SPEAKER_00'}, {'word': 'что', 'start': 39.438, 'end': 39.578, 'score': 0.996, 'speaker': 'SPEAKER_00'}, {'word': 'ты', 'start': 39.598, 'end': 39.638, 'score': 0.0, 'speaker': 'SPEAKER_00'}, {'word': 'на', 'start': 39.658, 'end': 39.718, 'score': 0.786, 'speaker': 'SPEAKER_00'}, {'word': 'голосовании', 'start': 39.738, 'end': 40.139, 'score': 0.881, 'speaker': 'SPEAKER_00'}, {'word': 'носил.', 'start': 40.199, 'end': 41.279, 'score': 0.995, 'speaker': 'SPEAKER_00'}], 'speaker': 'SPEAKER_00'}, {'start': 41.299, 'end': 42.819, 'text': 'Ну, хорошо.', 'words': [{'word': 'Ну,', 'start': 41.299, 'end': 41.359, 'score': 0.238}, {'word': 'хорошо.', 'start': 41.399, 'end': 42.819, 'score': 0.684, 'speaker': 'SPEAKER_02'}], 'speaker': 'SPEAKER_02'}, {'start': 42.879, 'end': 45.74, 'text': 'Так, мне хорошо слышно?', 'words': [{'word': 'Так,', 'start': 42.879, 'end': 43.119, 'score': 0.9, 'speaker': 'SPEAKER_02'}, {'word': 'мне', 'start': 43.179, 'end': 43.399, 'score': 0.931, 'speaker': 'SPEAKER_02'}, {'word': 'хорошо', 'start': 43.459, 'end': 43.719, 'score': 0.965, 'speaker': 'SPEAKER_02'}, {'word': 'слышно?', 'start': 43.739, 'end': 45.74, 'score': 0.612, 'speaker': 'SPEAKER_02'}], 'speaker': 'SPEAKER_02'}, {'start': 45.84, 'end': 45.9, 'text': 'Да.', 'words': [{'word': 'Да.', 'start': 45.84, 'end': 45.9, 'score': 0.612, 'speaker': 'SPEAKER_02'}], 'speaker': 'SPEAKER_02'}, {'start': 49.588, 'end': 51.229, 'text': ' Так, видно презентацию?', 'words': [{'word': 'Так,', 'start': 49.588, 'end': 49.848, 'score': 0.826, 'speaker': 'SPEAKER_02'}, {'word': 'видно', 'start': 50.289, 'end': 50.669, 'score': 0.975, 'speaker': 'SPEAKER_01'}, {'word': 'презентацию?', 'start': 50.729, 'end': 51.229, 'score': 0.626, 'speaker': 'SPEAKER_01'}], 'speaker': 'SPEAKER_01'}, {'start': 52.911, 'end': 54.972, 'text': 'Да.', 'words': [{'word': 'Да.', 'start': 52.911, 'end': 54.972, 'score': 0.978, 'speaker': 'SPEAKER_01'}], 'speaker': 'SPEAKER_01'}, {'start': 54.992, 'end': 58.155, 'text': 'Всем привет, меня зовут Большоков Тихон.', 'words': [{'word': 'Всем', 'start': 54.992, 'end': 55.212, 'score': 0.923, 'speaker': 'SPEAKER_01'}, {'word': 'привет,', 'start': 55.252, 'end': 55.813, 'score': 0.978, 'speaker': 'SPEAKER_01'}, {'word': 'меня', 'start': 55.853, 'end': 55.993, 'score': 0.655, 'speaker': 'SPEAKER_01'}, {'word': 'зовут', 'start': 56.033, 'end': 56.153, 'score': 0.363, 'speaker': 'SPEAKER_01'}, {'word': 'Большоков', 'start': 56.173, 'end': 56.533, 'score': 0.596, 'speaker': 'SPEAKER_01'}, {'word': 'Тихон.', 'start': 56.593, 'end': 58.155, 'score': 0.976, 'speaker': 'SPEAKER_01'}], 'speaker': 'SPEAKER_01'}, {'start': 58.215, 'end': 61.357, 'text': 'Сегодня я расскажу теорию EFusion Pro Ballistic Models, основные принципы', 'words': [{'word': 'Сегодня', 'start': 58.215, 'end': 58.455, 'score': 0.404, 'speaker': 'SPEAKER_01'}, {'word': 'я', 'start': 58.515, 'end': 58.875, 'score': 0.726, 'speaker': 'SPEAKER_01'}, {'word': 'расскажу', 'start': 58.935, 'end': 59.636, 'score': 0.979, 'speaker': 'SPEAKER_01'}, {'word': 'теорию', 'start': 59.696, 'end': 60.496, 'score': 0.93, 'speaker': 'SPEAKER_01'}, {'word': 'EFusion'}, {'word': 'Pro'}, {'word': 'Ballistic'}, {'word': 'Models,'}, {'word': 'основные', 'start': 60.616, 'end': 60.817, 'score': 0.124, 'speaker': 'SPEAKER_01'}, {'word': 'принципы', 'start': 60.837, 'end': 61.357, 'score': 0.514, 'speaker': 'SPEAKER_01'}], 'speaker': 'SPEAKER_01'}]\n"
     ]
    }
   ],
   "source": [
    "# 2. Align whisper output\n",
    "model_a, metadata = whisperx.load_align_model(language_code=RU, device=device)\n",
    "result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
    "print(result[\"segments\"]) # after alignment\n",
    "\n",
    "# delete model if low on GPU resources\n",
    "# import gc; gc.collect(); torch.cuda.empty_cache(); del model_a\n",
    "\n",
    "# 3. Assign speaker labels\n",
    "# Pipeline.from_pretrained('pyannote/speaker-diarization-3.1', use_auth_token=\"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\")\n",
    "diarize_model = whisperx.DiarizationPipeline(use_auth_token=\"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\", device=device)\n",
    "\n",
    "# add min/max number of speakers if known\n",
    "# Model.from_pretrained('pyannote/segmentation-3.0', use_auth_token=YOUR_AUTH_TOKEN)\n",
    "diarize_segments = diarize_model(audio)\n",
    "# diarize_model(audio, min_speakers=min_speakers, max_speakers=max_speakers)\n",
    "\n",
    "result = whisperx.assign_word_speakers(diarize_segments, result)\n",
    "print(diarize_segments)\n",
    "print(result[\"segments\"]) # segments are now assigned speaker IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 0.089,\n",
       " 'end': 1.11,\n",
       " 'text': ' Мы также продолжим записи.',\n",
       " 'words': [{'word': 'Мы',\n",
       "   'start': 0.089,\n",
       "   'end': 0.269,\n",
       "   'score': 0.524,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'word': 'также',\n",
       "   'start': 0.289,\n",
       "   'end': 0.529,\n",
       "   'score': 0.805,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'word': 'продолжим',\n",
       "   'start': 0.549,\n",
       "   'end': 0.83,\n",
       "   'score': 0.717,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'word': 'записи.',\n",
       "   'start': 0.87,\n",
       "   'end': 1.11,\n",
       "   'score': 0.873,\n",
       "   'speaker': 'SPEAKER_00'}],\n",
       " 'speaker': 'SPEAKER_00'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"segments\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SPEAKER_00'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"segments\"][0][\"speaker\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
